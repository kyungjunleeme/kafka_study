# Chapter 01. 카프카 시작하기

> **데이터를 이동시키는 작업에 더 적은 노력을 들일수록 핵심 비즈니스에 더욱 집중할 수 있다.**

## **1. 카프카란 무엇인가? 🤔**
- **발행/구독 메시지 전달 시스템**
- 메시지 큐를 관리하며 브로커를 통해 데이터를 발행자와 구독자 간에 중계
- 데이터 생성과 처리를 분리하고 효율적이고 유연하게 관리

### **1.1 발행/구독 시스템의 필요성**
- 전통적인 방식은 발행자와 구독자가 직접 연결되어 유지보수 비용이 높고 확장성이 낮음
- 발행/구독 모델은 데이터 흐름을 중앙 집중화하여 효율성과 확장성을 높임
- 카프카는 발행/구독 모델을 효과적으로 구현해 안정성과 성능을 제공


## **2. 카프카의 주요 개념 📌**

### **2.1 메시지와 배치**
- 카프카의 데이터 기본 단위는 **메시지(message) ✉️**
- 메시지는 키(key)와 값(value)로 구성
- 효율적인 처리를 위해 메시지를 **배치(batch)** 단위로 묶어서 전송
    - 배치 처리는 네트워크 I/O를 줄이고 처리 효율을 높임

### **2.2 스키마와 데이터 직렬화**
- 메시지는 단순 바이트 배열로 처리되지만, 구조 정의를 위해 스키마 사용 권장
- 아파치 에이브로(Avro)를 많이 활용
    - 스키마와 데이터를 분리해 데이터 변경 시에도 유연하게 처리
    - JSON과 유사하지만 더 조밀한 직렬화 형식 제공

### **2.3 토픽과 파티션**
- **토픽(Topic)**: 데이터 스트림을 나타내는 논리적 단위
- **파티션(Partition)**: 물리적으로 데이터를 저장하는 단위
    - 파티션 내에서는 메시지 순서 보장
    - 토픽 전체에서는 순서 보장하지 않음
- 파티션은 여러 서버에 분산 저장되어 확장성과 가용성을 제공


## **3. 카프카의 주요 구성 요소 📌**

### **3.1 프로듀서 (Producer)**
- 메시지를 생성해 특정 토픽에 기록
- 파티셔너(partitioner)를 통해 데이터를 파티션에 분배
    - 키 값을 해싱해 특정 파티션에 매핑가능
    - 동일 키 값을 가진 메시지는 같은 파티션에 저장돼 순서 보장
- 커스텀 파티셔너를 구현해 데이터 분배 방식 변경 가능

### **3.2 컨슈머 (Consumer)**
- 메시지를 읽고 처리하는 역할
- 하나 이상의 토픽을 구독하여 데이터를 순차적으로 가져옴
- 오프셋(offset)을 기록해 읽은 위치를 관리
    - 자동 또는 수동으로 오프셋 관리 가능
    - 필요 시 이전 오프셋으로 이동해 메시지 재처리 가능

### **3.3 브로커와 클러스터**
- **브로커**: 카프카 서버로 데이터를 저장하고 관리
- **클러스터**: 여러 브로커로 구성된 카프카 단위
    - **컨트롤러 브로커**: 파티션 할당과 브로커 관리 담당
    - **파티션 리더**: 파티션 데이터를 읽고 쓰는 작업 처리
    - **팔로워**: 리더 데이터를 복제해 장애 발생 시 리더 역할 수행
- 리더-팔로워 구조를 통해 데이터 복구 및 가용성 확보

### **3.4 데이터 보존 및 복제**
- 데이터를 **디스크 기반**으로 저장해 신뢰성 제공 💾
- 복제를 통해 데이터 가용성 확보
    - 리더-팔로워 구조로 리더 장애 시 팔로워가 리더로 전환
- 토픽별로 보존 기간과 저장 용량 설정 가능
    - ex. 일정 기간 후 데이터 삭제 또는 용량 초과 시 순차 삭제


## **4. 카프카의 주요 특징과 장점 📌**
- **다중 프로듀서 및 컨슈머 지원**
    - 여러 프로듀서와 컨슈머를 동시에 지원
    - 컨슈머 그룹으로 데이터를 병렬 처리 가능
- **높은 확장성**
    - 브로커 추가 및 파티션 분배로 처리량 확장 가능
- **디스크 기반 데이터 보존**
    - 데이터를 지속적으로 저장하여 장애 상황에서도 데이터 복구 가능


## **5. 카프카의 활용 사례 📰**
- **활동 추적**
    - 사용자 클릭 로그, 애플리케이션 사용 데이터 수집 및 분석
- **메시지 교환**
    - 서로 다른 시스템 간 데이터를 일정한 포맷으로 교환
- **지표 및 로그 수집**
    - 서버 로그와 지표 데이터를 실시간으로 수집
- **커밋 로그**
    - 데이터베이스 커밋 로그를 발행하여 실시간 변경 사항 전달
- **스트림 처리**
    - 데이터를 실시간으로 처리하여 분석 및 응답

## **6. 카프카의 기원 🧭**
- 링크드인의 모니터링 시스템 개선을 위해 시작
- 기존 문제: 긴 지표 수집 주기와 높은 유지보수 비용
- 카프카가 해결한 주요 요구사항
    - 발행자와 구독자 분리
    - 메시지 영속적 저장
    - 높은 메시지 처리량과 수평적 확장성 제공

## **핵심 정리 💡**
- 발행/구독 모델로 데이터 흐름을 중앙 집중화
- 토픽과 파티션으로 데이터를 효율적으로 분류 및 관리
- 에이브로(Avro) 등 스키마 기반으로 데이터 구조 정의
- 복제와 보존 기능으로 높은 가용성과 안정성 제공
- 높은 확장성과 다양한 활용 사례로 데이터 생태계에서 필수적인 도구
