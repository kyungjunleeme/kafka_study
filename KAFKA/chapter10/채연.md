# 10. 클러스터간 데이터 미러링하기
- 미러링: 카프카 클러스터 간의 데이터 복제
- 미러메이커: 아파치 카프카에서 클러스터간 데이터 복제를 수행하기 위한 툴
## 클러스터간 미러링 활용 사례
1. 지역 및 중앙 클러스터: 각각의 도시에 데이터센터를 가지고 카프카 클러스터를 설치함, 모든 데이터는 중앙 클러스터로 미러링됨
2. 고가용성과 재해 복구: 첫 번째 클러스터의 모든 데이터를 보유하는 여분의 두 번째 클러스터를 준비해서 가용성을 높임
3. 규제: 국가별로 다른 법적, 규제적 요구 조건을 따르기 위해 카프카 클러스터별로 서로 다른 설정과 정책을 시행해야 하는 경우
4. 클라우드 마이그레이션: 마이그레이션이 필요할 때 데이터센터 간의 트래픽을 관리하고 보안을 유지함
5. 엣지 클러스터로부터의 데이터 집적: IoT 기기에서 엣지 클러스터에 데이터를 저장하고 수집된 데이터를 집적해서 사용하는 경우
## 데이터센터간 통신을 할 때 고려해야 할 사항
1. 높은 지연: 두 카프카 클러스터 간의 거리, 네트워크 홉 개수
2. 제한된 대역폭
3. 더 높은 비용
아파치 카프카의 브로커와 클라이언트는 하나의 데이터센터 안에서 실행되도록 설계, 개발, 테스트, 조정되어있음
그래서 카프카 브로커를 서로 다른 데이터센터에 나눠서 설치하는 것은 권장되지 않음
## 아키텍쳐
1. 허브-앤-스포크 아키텍쳐
여러 개의 로컬 카프카 클러스터와 한 개의 중앙 카프카 클러스터가 존재함
데이터는 여러 개의 데이터센터에서 생성되지만, **일부 컨슈머는 전체 데이터를 사용해야 할 경우**사용됨
- 장점
  - 항상 로컬 데이터센터에서 데이터가 생성되고, 각각의 데이터센터에 저장된 이벤트가 중앙 데이터 센터로 단 한 번만 미러링됨 -> 미러링이 한 방향으로만 진행
- 단점
  - 지역 데이터센터에 있는 애플리케이션은 다른 데이터센터에 있는 데이터를 사용할 수 없다.
2. 액티브-액티브 아키텍쳐
액티브-액티브 아키텍쳐는 2개 이상의 데이터센터가 전체 데이터의 일부 혹은 전체를 공유하면서, 각 데이터센터가 모두 읽기와 쓰기를 수행할 수 있어야 할 경우 사용됨
- 장점
  - 인근 데이터센터에서 사용자들의 요청을 처리할 수 있음
  - 데이터 중복(중복이라기보단 이중화)으로 인해 회복 탄력성을 가짐
- 단점
  - 데이터를 여러 위치에서 비동기적으로 읽거나 변경할 경우 발생하는 충돌을 피하는 것이 어려움
  -> 순환 미러링 문제
    - 각 사용자를 특정 데이터센터에 고정시킨다.
    - 네임스페이스를 다르게 설정해서 순환 미러링을 방지한다.
    - 레코드 헤더(지원한다면) 사용
3. 액티브-스탠바이 아키텍쳐
특정한 상황의 재해 대비를 위해 스탠바이 클러스터는 액티브 상태 클러스터의 복사본을 평소에 가지고 있다가 비상시 스탠바이 클러스터를 작동하는 식으로 운영함
- 장점
  - 간단한 설치 가능
- 단점
  - 비용 문제(스탠바이 클러스터는 대기 상태)
  - 카프카 클러스터 간의 장애 복구가 보기보다 훨씬 어려운 일
4. 스트레치 클러스터
데이터센터 전체에 문제가 발생했을 때 카프카 클러스터에 장애가 발생하는 것을 방지 -> 하나의 카프카 클러스터를 여러 개의 데이터센터에 걸쳐 설치
## 아파치 카프카에서 DR(Disaster Recovery) 클러스터로 장애복구를 하기 위해 필요한 것들
1. 재해 복구 계획
  - 두 개의 지표를 염두에 두고 재해 복구 계획
  - RTO(복구 시간 목표): 모든 서비스가 장애가 발생한 뒤 다시 작동을 재개할 때까지의 최대 시간
  - RPO(복구 지점 목표): 장애의 결과로 인해 데이터가 유실될 수 있는 최대 시간
2. 계획에 없던 장애 복구에서의 데이터 유실과 불일치
  - 카프카 미러링 솔루션이 모두 비동기적으로 작동하기 때문에 요청이 매우 많은 시스템에서는 DR 클러스터가 주 클러스터에 비해 최소 수백, 수천 개의 메시지는 뒤쳐있을 수 있음
  - 사전에 계획된 장애 복구의 경우, 주 클러스터를 먼저 멈춘 뒤, 마이그레이션하기 전에 미러링 프로세스가 남은 메시지를 모두 미러링할 때까지 기다려서 유실 방지
  - 미러링 솔루션들이 현재로서는 트랜잭션을 지원하지 않으므로 여러 토픽들이 서로 연관되어있을 경우 데이터 정합성이 안맞을 수 있음
3. 장애 복구 이후 애플리케이션의 시작 오프셋
  다른 클러스터로 옮겨간 애플리케이션이 데이터를 읽어오기 시작해야하는 위치를 결정(시작 오프셋을 뭘로 설정할지)
  - 자동 오프셋 재설정: 처음부터 읽어올지 vs 맨 끝에서 시작할지(데이터 유실이 있을 수 있음 몇 개가 될지 모름)
  - 오프셋 토픽 복제: `__consumer_offests`토픽을 DR 클러스터로 미러링 해줌
    - 주의사항
      - 주 클러스터의 오프셋이 DR 클러스터의 오프셋과 일치할 것이라는 보장이 없음
      - 프로듀서 재시도로 인해 오프셋이 서로 달라질 수 있음
      - 설령 오프셋이 완벽히 보존되었다 할지라도, 주 클러스터와 DR 클러스터 사이에 랙이 존재함(트랜잭션 타이밍 문제일 수 있음)
  - 시간 기반 장애 복구: 카프카 0.10.0 버전부터 메시지는 카프카로 전송된 시각을 가리키는 타임스탬프 값을 가짐 이 타임스탬프를 기준으로 주어진 시각에 해당하는 오프셋을 가져오고, 해당 오프셋으로 이동한 뒤, 바로 그 지점에서부터 읽기 작업을 시작하면 됨
    - kafka-consumer-groups: 다양한 오프셋 초기화 지원
    ```bash
    bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --reset-offsets --all-topics --group my-group --to-datetime ~ --execute
    ```
  - 오프셋 변환: 주 클러스터와 DR 클러스터의 매핑되는 오프셋 정보를 외부 저장소에 저장하거나, 토픽에 저장하여 장애 복구할 때 저장된 오프셋 매핑 정보를 기준으로 복구 시작
4. 장애 복구가 끝난 후
- DR 클러스터가 주 클러스터가 되고, 미러링 프로세스의 방향만 반대로 바꾼다면 데이터의 일관성, 순서 보장 문제가 있음
- 이를 해결하기 위해 원래 주 클러스터의 저장된 데이터와 커밋된 오프셋을 완전히 삭제하고 새롭게 주 클러스터가 된 DR 클러스터의 내용을 다시 이전에 주 클러스터 였지만 DR 클러스터가 된 클러스터로 미러링해주는 방법이 있음
5. 클러스터 디스커버리 관련
- 주 클러스터에서 장애가 발생한 경우, 애플리케이션이 장애 복구용 클러스터와 통신을 시작하는 방법을 알 수 있게 하도록, 많은 조직에서 DNS를 사용해서 클러스터와 연결함
## 아파치 카프카의 미러메이커
- 초기 버전: 하나의 컨슈머 그룹에 속하는 여러 개의 컨슈머를 사용해서 복제할 토픽의 집합을 읽은 뒤, 미러메이커 프로세스들이 공유하는 카프카 프로듀서를 사용해서 읽어 온 이벤트를 목적지 클러스터에 쓰는 식으로 작동 -> 설정 변경시 지연값이 치솟는다던가, 새 토픽을 추가하면 리밸런스 때문에 멈추는 등의 문제 발생
- 미러메이커 2.0: **카프카 커넥터**프레임워크 기반하여 개발
  - 소스 커넥터 사용
  - 관리에 들어가는 수고를 줄임
  - 태스크에 파티션을 균등하게 배분함으로써 리밸런스로 인해 지연이 튀어오르는 상황 방지
  - 데이터 복제뿐만 아니라 컨슈머 오프켓, 토픽 설정, 토픽 ACL 마이그레이션까지 지원함
### 미러메이커 설정
```properties
clusters = NYC, LON // 클러스터 별칭 설정
NYC.bootstrap.servers = kafka.nyc.example.com:9092
LON.bootstrap.servers = kafka.lon.example.com:9092 // 각 클러스터에 대한 부트스트랩 서버 지정
NYC->LON.enabled = true // {원본}->{대상} 접두어를 사용해서 클러스 간에 복제 흐름 활성화
NYC->LON.topics = .* //복제 흐름에서 미러링 되는 토픽들을 정의
```
- 각 복제 흐름에서 미러링될 토픽들을 지정하기 위해 정규식을 사용할 수 있음
- 컨슈머 오프셋 마이그레이션
  - 미러메이커는 장애 복구를 수행할 때 주 클러스터에서 마지막으로 체크포인트된 오프셋을 DR 클러스터에서 찾을 수 있도록 RemoteClusterUtils 유틸리티 클래스를 포함함
  - 주기적으로 주 클러스터에 커밋된 오프셋을 자동으로 변환하여 DR zmffjtmxjdml __consumer_offsets에 커밋해 줌으로써 DR 클러스터로 옮겨가는 컨슈머들이 별도의 마이그레이션 작업 없이도 주 클러스터의 커밋 지점에 해당하는 오프셋에서 바로 작업을 재개할 수 있게 해줌
- 토픽 설정 및 ACL 마이그레이션
  - 기본 설정값을 그대로 두면, 일정한 주기로 ACL을 마이그레이션 해줌
- 커넥터 태스크
- 설정 접두어
### 미러메이커 보안
- 원본과 대상 클러스터 양쪽에 대해 보안이 적용된 브로커 리스너 사용
- 각 클러스터에 대해 클라이언트 쪽 보안 옵션들 설정 필수
- 모든 데이터센터간 트래픽에는 SSL 암호화 적용 필수
### 프로덕션 환경에서의 미러메이커
- 모니터링 해야할 것들
  - 카프카 커넥트
  - 미러메이커 지표
  - 랙 모니터링
    | 랙(lag): 원본 카프카 클러스터의 마지막 메시지 오프셋과 대상 카프카 클러스터의 마지막 메시지 오프셋 사이의 차이
  - 프로듀서/컨슈머 지표
  - 카나리아 테스트: 배포하기 전에 일부 시스템에 미리 배포해서 문제 없는지 확인하는 배포 기법
### 기타 클러스터간 미러링 솔루션
- 우버 uReplicator
- 링크드인 브루클린
- 컨플루언트 리플리케이터
